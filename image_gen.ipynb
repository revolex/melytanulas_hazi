{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5PPNww32GJT"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:32.118232Z",
     "start_time": "2024-11-05T10:48:32.111746Z"
    },
    "id": "ePAdpXuD2GJT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import Flowers102, OxfordIIITPet\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFYabStp2GJU"
   },
   "source": [
    "For reproducible results, we will be using a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:32.182979Z",
     "start_time": "2024-11-05T10:48:32.173268Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2Iw3qpW2GJU",
    "outputId": "8fa096b4-5fa2-4606-b294-f0b86f7e146c"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jN8as262GJU"
   },
   "source": [
    "### Data acquisition and analysis\n",
    "We will be using the Flowers102 and OxfordIIITPet dataset, which can be found in the torchvision library. The datasets will be downloaded from code if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:32.631300Z",
     "start_time": "2024-11-05T10:48:32.285129Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cjg3zOSD2GJV",
    "outputId": "7d2e7d49-9e3c-4e12-acc5-e6fddc245447"
   },
   "outputs": [],
   "source": [
    "flowers_dataset = Flowers102(root=\"./data\", split=\"train\", transform=None, download=True)\n",
    "pets_dataset = OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=None, download=True)\n",
    "print(flowers_dataset)\n",
    "print(pets_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21vfZK70BKOi"
   },
   "source": [
    "Let's see the class distribution and the resolution of the pictures in both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:32.681415Z",
     "start_time": "2024-11-05T10:48:32.676832Z"
    },
    "id": "jWFqDOzE2GJV"
   },
   "outputs": [],
   "source": [
    "def plot_class_size_distribution(dataset, dataset_name):\n",
    "    \n",
    "    classes = [label for _, label in dataset]\n",
    "    class_counts = Counter(classes)\n",
    "    class_sizes = list(class_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(class_sizes, bins=30, kde=False) \n",
    "    plt.title(f\"Class Size Distribution in {dataset_name} Dataset\")\n",
    "    plt.xlabel(\"Number of Samples per Class\")\n",
    "    plt.ylabel(\"Number of Classes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:42.023666Z",
     "start_time": "2024-11-05T10:48:32.714820Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-X8VgjN72rNp",
    "outputId": "76d8f0b3-97c9-4d84-c842-53a269c84155"
   },
   "outputs": [],
   "source": [
    "print(\"Flowers Dataset Class Distribution\")\n",
    "plot_class_size_distribution(flowers_dataset, \"Flowers102\")\n",
    "\n",
    "print(\"\\nPets Dataset Class Distribution\")\n",
    "plot_class_size_distribution(pets_dataset, \"Cars Pets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wdeVj_S5tyR"
   },
   "source": [
    "Based on the previous two diagrams, the flowers102 dataset has no discrepancies in terms of class distribution, and in the pets dataset we only have small discrepancies, which won't be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:42.068462Z",
     "start_time": "2024-11-05T10:48:42.062016Z"
    },
    "id": "OMeP0CqY2GJV"
   },
   "outputs": [],
   "source": [
    "def categorize_resolution(width, height):\n",
    "    if width < 256 and  height < 256:\n",
    "        return 'Low (<256)'\n",
    "    elif 256 <= width < 512 and 256 <= height < 512:\n",
    "        return 'Medium (256-512)'\n",
    "    elif 512 <= width < 1024 and 512 <= height < 1024:\n",
    "        return 'High (512-1024)'\n",
    "    else:\n",
    "      return 'Very High (>=1024)'\n",
    "\n",
    "\n",
    "def plot_image_size_distribution(dataset, dataset_name):\n",
    "    resolution_counts = {\n",
    "        'Low (<256)': 0,\n",
    "        'Medium (256-512)': 0,\n",
    "        'High (512-1024)': 0,\n",
    "        'Very High (>=1024)': 0\n",
    "    }\n",
    "    for img, _ in dataset:\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img)\n",
    "        category = categorize_resolution(img.size[0], img.size[1])\n",
    "        resolution_counts[category] += 1\n",
    "\n",
    "    categories = list(resolution_counts.keys())\n",
    "    counts = list(resolution_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(categories, counts, color='skyblue')\n",
    "    plt.title(f'Image Size Distribution of {dataset_name} by Resolution Category')\n",
    "    plt.xlabel('Resolution Category')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:49.966103Z",
     "start_time": "2024-11-05T10:48:42.084839Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "66SJbLVF3BWh",
    "outputId": "36c0c56f-a0ab-4baf-d58d-e00a7cff2ac6"
   },
   "outputs": [],
   "source": [
    "print(\"\\nFlowers102 Dataset Image Size Analysis\")\n",
    "plot_image_size_distribution(flowers_dataset, \"Flowers102\")\n",
    "\n",
    "print(\"\\nOxford-IIIT Pet Dataset Image Size Analysis\")\n",
    "plot_image_size_distribution(pets_dataset, \"Oxford Pets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weIDXgXdAPfN"
   },
   "source": [
    "We will be resizing the datasets to 256x256 resolution so they will be fine like this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHrF-XHv2GJV"
   },
   "source": [
    "### Data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BkcK0GbB05r"
   },
   "source": [
    "We will randomly split the datasets into 3 different parts, based on the following ratios:\n",
    "- Train: 70%,\n",
    "- Validation: 15%,\n",
    "- Test: 15%,\n",
    "\n",
    "because of our datasets don't have more than 10k samples. Also because of this, we will be loading the datasets to memory in one step rather than with a generator or streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:49.981703Z",
     "start_time": "2024-11-05T10:48:49.977137Z"
    },
    "id": "3vXlZukt2GJV"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    val_size = int(val_ratio * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    return random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:49.997587Z",
     "start_time": "2024-11-05T10:48:49.993814Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:50.034409Z",
     "start_time": "2024-11-05T10:48:50.009710Z"
    },
    "id": "OUhO-S-a4kL5"
   },
   "outputs": [],
   "source": [
    "flowers_dataset = Flowers102(root=\"./data\", split=\"train\", transform=train_transform, download=True)\n",
    "pets_dataset = OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=train_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:50.049385Z",
     "start_time": "2024-11-05T10:48:50.045294Z"
    },
    "id": "e2lWWkuD2GJW"
   },
   "outputs": [],
   "source": [
    "flowers_train, flowers_val, flowers_test = split_dataset(flowers_dataset)\n",
    "pets_train, pets_val, pets_test = split_dataset(pets_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:50.064901Z",
     "start_time": "2024-11-05T10:48:50.060537Z"
    },
    "id": "xxbUBM8X2GJW"
   },
   "outputs": [],
   "source": [
    "\n",
    "flowers102_train_loader = DataLoader(flowers_train, batch_size=64, shuffle=True)\n",
    "flowers102_val_loader = DataLoader(flowers_val, batch_size=64, shuffle=False)\n",
    "flowers102_test_loader = DataLoader(flowers_test, batch_size=64, shuffle=False)\n",
    "\n",
    "oxford_pets_train_loader = DataLoader(pets_train, batch_size=64, shuffle=True)\n",
    "oxford_pets_val_loader = DataLoader(pets_val, batch_size=64, shuffle=False)\n",
    "oxford_pets_test_loader = DataLoader(pets_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:50.084026Z",
     "start_time": "2024-11-05T10:48:50.078400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOQPGwPg2GJW",
    "outputId": "d0f78683-6da7-4e55-8f31-6690e04c2c65"
   },
   "outputs": [],
   "source": [
    "print(f\"Flowers102 dataset: {len(flowers_train)} training, {len(flowers_val)} validation, {len(flowers_test)} test samples\")\n",
    "print(f\"Oxford-IIIT Pets dataset: {len(pets_train)} training, {len(pets_val)} validation, {len(pets_test)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Baseline model\n",
    "\n",
    "We are implementing a simple VAE(Variational Autoencoder) so we can use it as a baseline model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:50.116588Z",
     "start_time": "2024-11-05T10:48:50.105361Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=2, learning_rate=1e-3):\n",
    "        super(VAE, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.enc = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 4, 2, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 32, 4, 2, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        output_shape = self._get_output_shape((3, 256, 256))\n",
    "\n",
    "        self.fc_mu = nn.Linear(output_shape, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(output_shape, latent_dim)\n",
    "        \n",
    "        vmi = output_shape / 32\n",
    "        vmi = int(math.sqrt(vmi))\n",
    "        \n",
    "        self.dec = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, output_shape),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Unflatten(1, (32, vmi, vmi)),\n",
    "            torch.nn.ConvTranspose2d(32, 32, 4, 2, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(32, 3, 4, 2, 1), \n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _get_output_shape(self, shape):\n",
    "        '''Returns the size of the output tensor from the conv layers.'''\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self.enc(input)\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_output = self.enc(x)\n",
    "        mu = self.fc_mu(enc_output)\n",
    "        logvar = self.fc_logvar(enc_output)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.dec(z), mu, logvar\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x_reconstructed, mu, logvar = self(x)\n",
    "        KL = 0.5 * torch.sum(mu**2 + torch.exp(logvar) - 1 - logvar)\n",
    "        reconstruction_loss = F.mse_loss(x_reconstructed, x, reduction=\"sum\")\n",
    "        loss = reconstruction_loss + KL\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x_reconstructed, mu, logvar = self(x)\n",
    "        KL = 0.5 * torch.sum(mu**2 + torch.exp(logvar) - 1 - logvar)\n",
    "        reconstruction_loss = F.mse_loss(x_reconstructed, x, reduction=\"sum\")\n",
    "        loss = reconstruction_loss + KL\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x_reconstructed, mu, logvar = self(x)\n",
    "        KL = 0.5 * torch.sum(mu**2 + torch.exp(logvar) - 1 - logvar)\n",
    "        reconstruction_loss = F.mse_loss(x_reconstructed, x, reduction=\"sum\")\n",
    "        loss = reconstruction_loss + KL\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:48:50.146256Z",
     "start_time": "2024-11-05T10:48:50.141989Z"
    }
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train two seperate models for the two datasets so we can later compare them by predefined metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:42.548099Z",
     "start_time": "2024-11-05T10:48:50.187536Z"
    }
   },
   "outputs": [],
   "source": [
    "model_flowers = VAE(latent_dim=64)\n",
    "model_pets = VAE(latent_dim=64)\n",
    "flower_trainer = pl.Trainer(max_epochs=10,accelerator='gpu',devices=1)\n",
    "pets_trainer = pl.Trainer(max_epochs=10,accelerator='gpu',devices=1)\n",
    "flower_trainer.fit(model_flowers, flowers102_train_loader)\n",
    "pets_trainer.fit(model_pets,oxford_pets_train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some of the pictures and their reconstructed counterparts from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:42.557201Z",
     "start_time": "2024-11-05T10:51:42.551112Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def unnormalize(img):\n",
    "    return (img + 1) / 2\n",
    "\n",
    "def plot_reconstructions(model, data_loader, num_images=10):\n",
    "    model.eval()\n",
    "    images, _ = next(iter(data_loader))\n",
    "    images = images[:num_images] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon_images, _, _ = model(images.to(model.device))\n",
    "    \n",
    "    recon_images = recon_images.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(15, 4))\n",
    "    for i in range(num_images):\n",
    "     \n",
    "        axes[0, i].imshow(unnormalize(images[i]).permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        axes[1, i].imshow(unnormalize(recon_images[i]).permute(1, 2, 0).cpu().numpy())\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:44.041850Z",
     "start_time": "2024-11-05T10:51:42.568315Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(model_flowers, flowers102_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:45.637504Z",
     "start_time": "2024-11-05T10:51:44.069722Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(model_pets, oxford_pets_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defining Evaluation Criteria\n",
    "\n",
    "For evaluating the quality of generated images from the diffusion models, we will use the following metrics:\n",
    "\n",
    "1. **Fr√©chet Inception Distance (FID)**: Measures the similarity between generated images and real images by comparing the mean and covariance of features extracted from the Inception network. Lower values are better, indicating closer similarity to real data.\n",
    "\n",
    "2. **Inception Score (IS)**: Measures the quality of generated images based on their diversity and how \"confident\" the Inception network is in classifying them into distinct categories. Higher scores indicate better diversity and quality.\n",
    "\n",
    "We will implement and calculate these metrics after training the diffusion models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:45.658753Z",
     "start_time": "2024-11-05T10:51:45.653125Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_fid(real_images_loader, model):\n",
    "    model = model.to(\"cuda\")\n",
    "    fid_metric = FrechetInceptionDistance().to(\"cuda\")\n",
    "    \n",
    "    \n",
    "    for real_images, _ in real_images_loader:\n",
    "        real_images = (real_images * 255).byte() \n",
    "        fid_metric.update(real_images.to(\"cuda\"), real=True)\n",
    "    \n",
    "    for real_images, _ in real_images_loader:\n",
    "        real_images = real_images.to(\"cuda\")  \n",
    "        generated_images, _, _ = model(real_images) \n",
    "        generated_images = (generated_images * 255).clamp(0, 255).byte() \n",
    "        fid_metric.update(generated_images, real=False)  \n",
    "    \n",
    "    return fid_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:45.673122Z",
     "start_time": "2024-11-05T10:51:45.668657Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_inception_score(real_images_loader, model):\n",
    "    model = model.to(\"cuda\")\n",
    "    inception_score = InceptionScore().to(\"cuda\")\n",
    "   \n",
    "    for real_images, _ in real_images_loader:\n",
    "        generated_images, _, _ = model(real_images.to(\"cuda\"))  \n",
    "        generated_images = (generated_images * 255).clamp(0, 255).byte()\n",
    "        inception_score.update(generated_images)\n",
    "\n",
    "    score = inception_score.compute()\n",
    "    return score[0].item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:51:51.631149Z",
     "start_time": "2024-11-05T10:51:45.683657Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fid_value = calculate_fid(flowers102_val_loader, model_flowers)\n",
    "print(\"FID:\", fid_value)\n",
    "\n",
    "inception_score_value = calculate_inception_score(flowers102_val_loader, model_flowers)\n",
    "print(\"Inception Score:\", inception_score_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated metrics give us the clue that the baseline model isn't performing really well. However, this kind of baseline will be perfect for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:52:02.799411Z",
     "start_time": "2024-11-05T10:51:51.701454Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fid_value = calculate_fid(oxford_pets_val_loader, model_pets)\n",
    "print(\"FID:\", fid_value)\n",
    "\n",
    "inception_score_value = calculate_inception_score(oxford_pets_val_loader, model_pets)\n",
    "print(\"Inception Score:\", inception_score_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results a bit better for this dataset, this is most likely due to that larger sample size available in this dataset compared to the other one."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
